import requests
from bs4 import BeautifulSoup
import re

def search_tonkiang(query):
    url = 'http://tonkiang.us/'
    headers = {
        'Host': 'tonkiang.us',
        'Connection': 'keep-alive',
        'Cache-Control': 'max-age=0',
        'Origin': 'http://tonkiang.us',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Referer': 'http://tonkiang.us/?',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    data = {
        'seerch': query,
        'Submit': '+',
        'city': 'NjA2NTM0MzcyMzc4a2tr'
    }

    cookies = {
        'REFERER': 'Gameover',
        # ... (çœç•¥å…¶ä»– cookie)
    }

    response = requests.post(url, headers=headers, data=data, cookies=cookies)

    if response.status_code == 200:
        print("è¯·æ±‚æˆåŠŸï¼Œæ­£åœ¨è§£æé¡µé¢å†…å®¹...")
        return response.text  # è¿”å› HTML å†…å®¹ä»¥ä¾›åç»­è§£æ
    else:
        print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return ""

def extract_channel_info(html_content):
    result = []
    soup = BeautifulSoup(html_content, 'html.parser')
    resultplus_list = soup.find_all('div', class_='resultplus')
    
    if not resultplus_list:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢‘é“ä¿¡æ¯ã€‚")
    
    for resultplus in resultplus_list:
        # æå–é¢‘é“åç§°
        channel_name_tag = resultplus.find('div', class_='tip')
        channel_name = channel_name_tag.text.strip() if channel_name_tag else "æœªçŸ¥é¢‘é“"

        # æŸ¥æ‰¾æ‰€æœ‰ img æ ‡ç­¾å¹¶æå– onclick å±æ€§
        copy_buttons = resultplus.find_all('img', style=re.compile('cursor:pointer;'))
        for img in copy_buttons:
            onclick_value = img.get('onclick', '')
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– URL
            match = re.search(r'iryae\("([^"]+)"\)', onclick_value)
            if match:
                play_url = match.group(1)
                result.append((channel_name, play_url))
            else:
                print(f"æœªèƒ½ä» onclick ä¸­æå– URL: {onclick_value}")

    return result

def format_output(results):
    output = ["ğŸ’˜ä¸­å¤®,#genre#"]
    channel_dict = {}

    # å°†ç»“æœåˆ†ç»„
    for channel_name, link in results:
        if channel_name not in channel_dict:
            channel_dict[channel_name] = []
        channel_dict[channel_name].append(link)

    # ç”Ÿæˆæœ€ç»ˆè¾“å‡ºæ ¼å¼
    for channel_name, links in channel_dict.items():
        for link in links[:5]:  # åªå–å‰5æ¡é“¾æ¥
            output.append(f"{channel_name},{link}")

    return output

def main():
    print("è¯·é€‰æ‹©æœç´¢é€‰é¡¹:")
    print("1. é»˜è®¤æœç´¢ CCTV-1 åˆ° CCTV-10")
    print("2. è‡ªå®šä¹‰æœç´¢å†…å®¹")

    choice = input("è¯·è¾“å…¥é€‰é¡¹ (1 æˆ– 2): ")
    results = []

    if choice == '1':
        for i in range(1, 11):
            query = f"CCTV-{i}"
            print(f"æœç´¢: {query}")
            html_content = search_tonkiang(query)
            results.extend(extract_channel_info(html_content))
    elif choice == '2':
        query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼š")
        html_content = search_tonkiang(query)
        results.extend(extract_channel_info(html_content))
    else:
        print("æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·è¾“å…¥ 1 æˆ– 2ã€‚")

    # æ ¼å¼åŒ–è¾“å‡º
    formatted_output = format_output(results)
    for line in formatted_output:
        print(line)

if __name__ == "__main__":
    main()